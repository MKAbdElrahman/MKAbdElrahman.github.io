<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Software Architecture on Mohamed Abdelrahman</title>
    <link>http://localhost:1313/tags/software-architecture/</link>
    <description>Recent content in Software Architecture on Mohamed Abdelrahman</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 09 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/software-architecture/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Sagas Quantatively: Fantasy Fiction</title>
      <link>http://localhost:1313/posts/saga-fantasyfiction/</link>
      <pubDate>Tue, 09 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/saga-fantasyfiction/</guid>
      <description>&lt;p&gt;Last time, we discussed the first saga cataloged by Mark Richards and Neal Ford in their book, &amp;ldquo;Software Architecture: The Hard Parts&amp;rdquo;. We saw how the epic saga is exponentially sensitive to the availabilities of the orchestrator and the orchestrated services. We also noted how latency worsens with increasing availability due to the sequential blocking model.  Today, we will discuss the second saga they called the fantasy fiction saga. In this saga, we switch from a blocking communication model to a non-blocking one while maintaining the atomicity constraint.  In a previous post, I discussed why we should avoid using the terms synchronous and asynchronous, and instead use more explicit terms like blocking request-reply, non-blocking request-reply, and event-driven. Let’s apply these concepts to the fantasy fiction saga.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sagas Quantatively: Epic Saga</title>
      <link>http://localhost:1313/posts/saga-epic/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/saga-epic/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/saga/epic-saga.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;When we transition to microservices, each service owns its data. Achieving transactions across multiple services isn&amp;rsquo;t as straightforward as with a single monolithic database.&lt;/p&gt;&#xA;&lt;p&gt;One way to address this challenge is through sagas, which simulate distributed transactions using a series of local transactions. This approach was popularized by Chris Richardson and later analyzed by Mark Richards and Neal Ford in their book &amp;lsquo;Sotware Architecture: The Hard Parts&amp;rsquo;.&lt;/p&gt;&#xA;&lt;p&gt;They examined how these factors affect saga characteristics:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Microservices: Why Not a Shared Monolithic Database?</title>
      <link>http://localhost:1313/posts/why-not-shared-db/</link>
      <pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/why-not-shared-db/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/microservices/shared-db.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;In microservices, the idea of using a shared monolithic database is always rejected. There are several reasons why this approach is not advisable. Below are the key considerations that highlight the drawbacks of a shared monolithic database.&lt;/p&gt;&#xA;&lt;h3 id=&#34;change-control&#34;&gt;Change Control&lt;/h3&gt;&#xA;&lt;p&gt;Managing changes in a shared monolithic database is a complex task. Any schema change necessitates coordination across all dependent services, which can be cumbersome and time-consuming. This coordination is required to ensure that all services are redeployed simultaneously to prevent schema mismatch errors, which can cause significant downtime and service disruption.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Sharing Between Microservices Using Synchronized Embedded Caches</title>
      <link>http://localhost:1313/posts/caching_data-shraing-embedded-cache/</link>
      <pubDate>Thu, 27 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/caching_data-shraing-embedded-cache/</guid>
      <description>&lt;h3 id=&#34;setting-the-scene&#34;&gt;Setting the Scene&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/caching/data-sharing-embedded.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Consider an online food delivery system with two essential microservices:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Menu Service&lt;/strong&gt;: Manages the list of available dishes.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Order Service&lt;/strong&gt;: Allows customers to create and manage their orders.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;how-it-works&#34;&gt;How It Works&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Starting the Menu Service&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The menu service starts and loads a cache containing information about available dishes.&lt;/li&gt;&#xA;&lt;li&gt;This cache is distributed, enabling it to be shared across different services.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Starting the Order Service&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;When the order service starts, it recognizes the cache used by the menu service.&lt;/li&gt;&#xA;&lt;li&gt;The system facilitates a handshake between the services, allowing them to join the same cluster and establish a connection.&lt;/li&gt;&#xA;&lt;li&gt;Both services now know about each other. The menu service’s cache includes the list of dishes, and the order service synchronizes with this cache.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;demonstrating-synchronization&#34;&gt;Demonstrating Synchronization&lt;/h3&gt;&#xA;&lt;p&gt;Here’s how the two services interact:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Caching: Topologies and Patterns</title>
      <link>http://localhost:1313/posts/caching_patterns/</link>
      <pubDate>Wed, 26 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/caching_patterns/</guid>
      <description>&lt;h2 id=&#34;what-is-a-cache&#34;&gt;What is a Cache?&lt;/h2&gt;&#xA;&lt;p&gt;A cache is a secondary data store that’s faster to read from than the data’s primary store. The purpose of a cache is to improve application performance by:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reducing network calls to the primary data store:&lt;/strong&gt; Instead of repeatedly querying the database, the cache handles data retrieval, offloading processing from the database. This reduces database connection consumption, often a limiting factor, and allows the database to perform faster.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Increasing performance:&lt;/strong&gt; Data in a cache is typically stored in random-access memory (RAM), providing quicker access than querying a database.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;In distributed databases, the performance enhancement might be marginal due to network call latency, but the increase in scalability can be significant.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
